{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee27e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196939a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a20798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNpy(filename):\n",
    "    with open(filename, \"rb\") as f: return np.load(f)\n",
    "\n",
    "db = f\"{os.getcwd()}/../data/slices/gaussian-grabber/size160000-hw40-d100/\"\n",
    "data_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e26cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = loadNpy(db + f\"X_train.npy\"), loadNpy(db + f\"X_val.npy\"), loadNpy(db + f\"X_test.npy\")\n",
    "y_train, y_val, y_test = loadNpy(db + f\"c-y{data_type}_train.npy\"), loadNpy(db + f\"c-y{data_type}_val.npy\"), loadNpy(db + f\"c-y{data_type}_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb3f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_to_tensor(data, l=False):\n",
    "    if not l:\n",
    "        data = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        data = torch.from_numpy(data).type(torch.LongTensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82979bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = npy_to_tensor(X_train), npy_to_tensor(X_val), npy_to_tensor(X_test)\n",
    "y_train, y_val, y_test = npy_to_tensor(y_train, 1), npy_to_tensor(y_val, 1), npy_to_tensor(y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e89162",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b11393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loader\n",
    "train_iterator = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "val_iterator = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_iterator = DataLoader(dataset=test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5a33e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1200, 160000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34ef275",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, dtype=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49027/1188773190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3415\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3417\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3419\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[0;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, dtype=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce6d7ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18666666666666668"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6e32fd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15666666666666668"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36e3c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 160000\n",
    "h1_size = 20000\n",
    "h2_size = 100\n",
    "h3_size = 100\n",
    "h4_size = 100\n",
    "h5_size = 100\n",
    "num_classes = 2\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199e0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1ca0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, h1_size).to(device)\n",
    "        #self.bn1 = nn.BatchNorm1d(h1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(h1_size, h2_size).to(device)\n",
    "        #self.bn2 = nn.BatchNorm1d(h2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #self.drop1 = nn.Dropout(p=0.1)\n",
    "        #self.bn2 = nn.BatchNorm1d(h2_size)\n",
    "        self.fc3 = nn.Linear(h2_size, h3_size).to(device)\n",
    "        #self.bn3 = nn.BatchNorm1d(h3_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #self.bn3 = nn.BatchNorm1d(h3_size)\n",
    "        self.fc4 = nn.Linear(h3_size, h4_size).to(device)\n",
    "        #self.bn4 = nn.BatchNorm1d(h4_size)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #self.bn4 = nn.BatchNorm1d(h4_size)\n",
    "        #self.drop2 = nn.Dropout(p=0.1)\n",
    "        self.fc5 = nn.Linear(h4_size, h5_size).to(device)\n",
    "        #self.bn5 = nn.BatchNorm1d(h4_size)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.drop5 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc6 = nn.Linear(h2_size, num_classes).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        #out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        #out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        #out = self.bn2(out)\n",
    "        out = self.fc3(out)\n",
    "        #out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.drop3(out)\n",
    "        #out = self.bn3(out)\n",
    "        \n",
    "        out = self.fc4(out)\n",
    "        #out = self.bn4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.drop4(out)\n",
    "        \n",
    "        #out = self.bn4(out)\n",
    "        out = self.fc5(out)\n",
    "        #out = self.bn5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.drop5(out)\n",
    "        \n",
    "        out = self.fc6(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd2b83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nn(mod, data):\n",
    "    preds = []\n",
    "    actual = []\n",
    "    \n",
    "    shape = data.dataset.tensors[0].shape[1:]\n",
    "    \n",
    "    mod.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = mod(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds.append(predicted)\n",
    "            actual.append(labels)\n",
    "\n",
    "        print(f'Accuracy of the network on the {total} test images: {100 * correct / total} %')\n",
    "    return preds, actual, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5307e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NeuralNet(input_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "474699ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19c219ea11f4ae4a27fb5ba3081099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [19/19], Loss: 99.9551\n",
      "Accuracy of the network on the 1200 test images: 84.66666666666667 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [2/1000], Step [19/19], Loss: 4.5253\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [3/1000], Step [19/19], Loss: 0.6380\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [4/1000], Step [19/19], Loss: 0.6158\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [5/1000], Step [19/19], Loss: 1.2726\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [6/1000], Step [19/19], Loss: 0.5938\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [7/1000], Step [19/19], Loss: 0.5816\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [8/1000], Step [19/19], Loss: 3.9584\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [9/1000], Step [19/19], Loss: 0.5721\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [10/1000], Step [19/19], Loss: 0.8306\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [11/1000], Step [19/19], Loss: 0.5609\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [12/1000], Step [19/19], Loss: 0.5491\n",
      "Accuracy of the network on the 1200 test images: 84.58333333333333 %\n",
      "Accuracy of the network on the 400 test images: 86.75 %\n",
      "Accuracy of the network on the 400 test images: 84.25 %\n",
      "Epoch [13/1000], Step [19/19], Loss: 0.5456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49763/4207555113.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n\u001b[1;32m     24\u001b[0m            .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\u001b[0;32m---> 25\u001b[0;31m     nn_epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_nn(nn_model, train_iterator), \n\u001b[0m\u001b[1;32m     26\u001b[0m                  \"val\": eval_nn(nn_model, val_iterator), \"test\": eval_nn(nn_model, test_iterator)}\n\u001b[1;32m     27\u001b[0m     \u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49763/3169449419.py\u001b[0m in \u001b[0;36meval_nn\u001b[0;34m(mod, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_epochs = {}\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_iterator)\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (images, labels) in enumerate(train_iterator):  \n",
    "        # Move tensors to the configured device\n",
    "        #print(images.shape)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        #print(images.shape)\n",
    "        outputs = nn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(i)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "    nn_epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_nn(nn_model, train_iterator), \n",
    "                 \"val\": eval_nn(nn_model, val_iterator), \"test\": eval_nn(nn_model, test_iterator)}\n",
    "    nn_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94228bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_nn(nn_model, val_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffe76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86ba484",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': train_iterator,\n",
    "    'val': val_iterator,\n",
    "    'test': test_iterator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5aa612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=40,              \n",
    "                out_channels=64,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        ).to(device)\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 64, 5, 1, 2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.MaxPool2d(2),                \n",
    "        ).to(device)\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 256, 5, 1, 2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            #nn.MaxPool2d(3),\n",
    "        ).to(device)\n",
    "        \"\"\"\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            #nn.MaxPool2d(2),\n",
    "        ).to(device)\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(128, 512, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1)\n",
    "        ).to(device)\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "        self.conv8 = nn.Sequential(         \n",
    "            nn.Conv2d(256, 128, 5, 1, 2),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            #nn.MaxPool2d(2),                \n",
    "        ).to(device)\n",
    "        self.conv9 = nn.Sequential(         \n",
    "            nn.Conv2d(128, 512, 5, 1, 2),\n",
    "            #nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            #nn.MaxPool2d(3),\n",
    "        ).to(device)\n",
    "        self.conv10 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(p=0.1),\n",
    "            #nn.MaxPool2d(2),\n",
    "        ).to(device)\n",
    "        self.conv11 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "        \"\"\"\n",
    "        \n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(64000, 3).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \"\"\"\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.conv11(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb94006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ROC\n",
    "\n",
    "def eval_cnn(data):\n",
    "    \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred_ys = []\n",
    "        labs = []\n",
    "        for images, labels in loaders[data]:\n",
    "            images = images.reshape(-1,40,40,100).to(device)\n",
    "            labels = labels.to(device)\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            pred_ys = pred_ys + test_output.flatten().tolist()\n",
    "            labs = labs + labels.tolist()\n",
    "            total += len(labels)\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            pass\n",
    "        print(f'{data} Accuracy of the model on the {total} {data} images: %.3f' % (correct/total))\n",
    "        return pred_ys, labs, correct/total\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c45a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(40, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(64, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (out): Linear(in_features=64000, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc4574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15000d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.001)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32718eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1174be455a44e5a62849107323368e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [1/12], Loss: 84.3675\n",
      "train Accuracy of the model on the 1200 train images: 0.396\n",
      "val Accuracy of the model on the 400 val images: 0.400\n",
      "test Accuracy of the model on the 400 test images: 0.400\n",
      "Epoch [1/1000], Step [11/12], Loss: 5.2603\n",
      "train Accuracy of the model on the 1200 train images: 0.411\n",
      "val Accuracy of the model on the 400 val images: 0.417\n",
      "test Accuracy of the model on the 400 test images: 0.398\n",
      "Epoch [2/1000], Step [1/12], Loss: 2.5850\n",
      "train Accuracy of the model on the 1200 train images: 0.215\n",
      "val Accuracy of the model on the 400 val images: 0.195\n",
      "test Accuracy of the model on the 400 test images: 0.220\n",
      "Epoch [2/1000], Step [11/12], Loss: 1.7068\n",
      "train Accuracy of the model on the 1200 train images: 0.385\n",
      "val Accuracy of the model on the 400 val images: 0.357\n",
      "test Accuracy of the model on the 400 test images: 0.367\n",
      "Epoch [3/1000], Step [1/12], Loss: 1.2692\n",
      "train Accuracy of the model on the 1200 train images: 0.444\n",
      "val Accuracy of the model on the 400 val images: 0.448\n",
      "test Accuracy of the model on the 400 test images: 0.393\n",
      "Epoch [3/1000], Step [11/12], Loss: 1.1186\n",
      "train Accuracy of the model on the 1200 train images: 0.451\n",
      "val Accuracy of the model on the 400 val images: 0.455\n",
      "test Accuracy of the model on the 400 test images: 0.405\n",
      "Epoch [4/1000], Step [1/12], Loss: 1.1073\n",
      "train Accuracy of the model on the 1200 train images: 0.486\n",
      "val Accuracy of the model on the 400 val images: 0.458\n",
      "test Accuracy of the model on the 400 test images: 0.415\n",
      "Epoch [4/1000], Step [11/12], Loss: 0.9617\n",
      "train Accuracy of the model on the 1200 train images: 0.473\n",
      "val Accuracy of the model on the 400 val images: 0.465\n",
      "test Accuracy of the model on the 400 test images: 0.420\n",
      "Epoch [5/1000], Step [1/12], Loss: 1.0317\n",
      "train Accuracy of the model on the 1200 train images: 0.490\n",
      "val Accuracy of the model on the 400 val images: 0.490\n",
      "test Accuracy of the model on the 400 test images: 0.420\n",
      "Epoch [5/1000], Step [11/12], Loss: 0.9247\n",
      "train Accuracy of the model on the 1200 train images: 0.549\n",
      "val Accuracy of the model on the 400 val images: 0.480\n",
      "test Accuracy of the model on the 400 test images: 0.425\n",
      "Epoch [6/1000], Step [1/12], Loss: 0.9287\n",
      "train Accuracy of the model on the 1200 train images: 0.511\n",
      "val Accuracy of the model on the 400 val images: 0.507\n",
      "test Accuracy of the model on the 400 test images: 0.430\n",
      "Epoch [6/1000], Step [11/12], Loss: 0.8574\n",
      "train Accuracy of the model on the 1200 train images: 0.634\n",
      "val Accuracy of the model on the 400 val images: 0.465\n",
      "test Accuracy of the model on the 400 test images: 0.420\n",
      "Epoch [7/1000], Step [1/12], Loss: 0.7949\n",
      "train Accuracy of the model on the 1200 train images: 0.593\n",
      "val Accuracy of the model on the 400 val images: 0.487\n",
      "test Accuracy of the model on the 400 test images: 0.410\n",
      "Epoch [7/1000], Step [11/12], Loss: 0.8136\n",
      "train Accuracy of the model on the 1200 train images: 0.648\n",
      "val Accuracy of the model on the 400 val images: 0.470\n",
      "test Accuracy of the model on the 400 test images: 0.430\n",
      "Epoch [8/1000], Step [1/12], Loss: 0.7896\n",
      "train Accuracy of the model on the 1200 train images: 0.637\n",
      "val Accuracy of the model on the 400 val images: 0.495\n",
      "test Accuracy of the model on the 400 test images: 0.427\n",
      "Epoch [8/1000], Step [11/12], Loss: 0.7585\n",
      "train Accuracy of the model on the 1200 train images: 0.713\n",
      "val Accuracy of the model on the 400 val images: 0.453\n",
      "test Accuracy of the model on the 400 test images: 0.412\n",
      "Epoch [9/1000], Step [1/12], Loss: 0.7027\n",
      "train Accuracy of the model on the 1200 train images: 0.718\n",
      "val Accuracy of the model on the 400 val images: 0.472\n",
      "test Accuracy of the model on the 400 test images: 0.422\n",
      "Epoch [9/1000], Step [11/12], Loss: 0.7049\n",
      "train Accuracy of the model on the 1200 train images: 0.767\n",
      "val Accuracy of the model on the 400 val images: 0.468\n",
      "test Accuracy of the model on the 400 test images: 0.417\n",
      "Epoch [10/1000], Step [1/12], Loss: 0.6182\n",
      "train Accuracy of the model on the 1200 train images: 0.772\n",
      "val Accuracy of the model on the 400 val images: 0.463\n",
      "test Accuracy of the model on the 400 test images: 0.443\n",
      "Epoch [10/1000], Step [11/12], Loss: 0.6098\n",
      "train Accuracy of the model on the 1200 train images: 0.814\n",
      "val Accuracy of the model on the 400 val images: 0.455\n",
      "test Accuracy of the model on the 400 test images: 0.412\n",
      "Epoch [11/1000], Step [1/12], Loss: 0.5703\n",
      "train Accuracy of the model on the 1200 train images: 0.835\n",
      "val Accuracy of the model on the 400 val images: 0.455\n",
      "test Accuracy of the model on the 400 test images: 0.432\n",
      "Epoch [11/1000], Step [11/12], Loss: 0.5784\n",
      "train Accuracy of the model on the 1200 train images: 0.827\n",
      "val Accuracy of the model on the 400 val images: 0.450\n",
      "test Accuracy of the model on the 400 test images: 0.420\n",
      "Epoch [12/1000], Step [1/12], Loss: 0.4697\n",
      "train Accuracy of the model on the 1200 train images: 0.862\n",
      "val Accuracy of the model on the 400 val images: 0.443\n",
      "test Accuracy of the model on the 400 test images: 0.453\n",
      "Epoch [12/1000], Step [11/12], Loss: 0.4899\n",
      "train Accuracy of the model on the 1200 train images: 0.872\n",
      "val Accuracy of the model on the 400 val images: 0.425\n",
      "test Accuracy of the model on the 400 test images: 0.417\n",
      "Epoch [13/1000], Step [1/12], Loss: 0.4173\n",
      "train Accuracy of the model on the 1200 train images: 0.887\n",
      "val Accuracy of the model on the 400 val images: 0.458\n",
      "test Accuracy of the model on the 400 test images: 0.432\n",
      "Epoch [13/1000], Step [11/12], Loss: 0.4351\n",
      "train Accuracy of the model on the 1200 train images: 0.887\n",
      "val Accuracy of the model on the 400 val images: 0.432\n",
      "test Accuracy of the model on the 400 test images: 0.405\n",
      "Epoch [14/1000], Step [1/12], Loss: 0.3418\n",
      "train Accuracy of the model on the 1200 train images: 0.922\n",
      "val Accuracy of the model on the 400 val images: 0.425\n",
      "test Accuracy of the model on the 400 test images: 0.412\n",
      "Epoch [14/1000], Step [11/12], Loss: 0.3469\n",
      "train Accuracy of the model on the 1200 train images: 0.886\n",
      "val Accuracy of the model on the 400 val images: 0.420\n",
      "test Accuracy of the model on the 400 test images: 0.393\n",
      "Epoch [15/1000], Step [1/12], Loss: 0.3200\n",
      "train Accuracy of the model on the 1200 train images: 0.937\n",
      "val Accuracy of the model on the 400 val images: 0.427\n",
      "test Accuracy of the model on the 400 test images: 0.422\n",
      "Epoch [15/1000], Step [11/12], Loss: 0.2997\n",
      "train Accuracy of the model on the 1200 train images: 0.873\n",
      "val Accuracy of the model on the 400 val images: 0.412\n",
      "test Accuracy of the model on the 400 test images: 0.398\n",
      "Epoch [16/1000], Step [1/12], Loss: 0.2945\n",
      "train Accuracy of the model on the 1200 train images: 0.919\n",
      "val Accuracy of the model on the 400 val images: 0.420\n",
      "test Accuracy of the model on the 400 test images: 0.380\n",
      "Epoch [16/1000], Step [11/12], Loss: 0.3246\n",
      "train Accuracy of the model on the 1200 train images: 0.910\n",
      "val Accuracy of the model on the 400 val images: 0.443\n",
      "test Accuracy of the model on the 400 test images: 0.425\n",
      "Epoch [17/1000], Step [1/12], Loss: 0.2348\n",
      "train Accuracy of the model on the 1200 train images: 0.812\n",
      "val Accuracy of the model on the 400 val images: 0.328\n",
      "test Accuracy of the model on the 400 test images: 0.335\n",
      "Epoch [17/1000], Step [11/12], Loss: 0.4790\n",
      "train Accuracy of the model on the 1200 train images: 0.787\n",
      "val Accuracy of the model on the 400 val images: 0.438\n",
      "test Accuracy of the model on the 400 test images: 0.420\n",
      "Epoch [18/1000], Step [1/12], Loss: 0.5255\n",
      "train Accuracy of the model on the 1200 train images: 0.773\n",
      "val Accuracy of the model on the 400 val images: 0.385\n",
      "test Accuracy of the model on the 400 test images: 0.393\n",
      "Epoch [18/1000], Step [11/12], Loss: 0.7511\n",
      "train Accuracy of the model on the 1200 train images: 0.762\n",
      "val Accuracy of the model on the 400 val images: 0.477\n",
      "test Accuracy of the model on the 400 test images: 0.455\n",
      "Epoch [19/1000], Step [1/12], Loss: 0.5864\n",
      "train Accuracy of the model on the 1200 train images: 0.799\n",
      "val Accuracy of the model on the 400 val images: 0.458\n",
      "test Accuracy of the model on the 400 test images: 0.435\n",
      "Epoch [19/1000], Step [11/12], Loss: 0.4823\n",
      "train Accuracy of the model on the 1200 train images: 0.884\n",
      "val Accuracy of the model on the 400 val images: 0.477\n",
      "test Accuracy of the model on the 400 test images: 0.460\n",
      "Epoch [20/1000], Step [1/12], Loss: 0.2343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Accuracy of the model on the 1200 train images: 0.875\n",
      "val Accuracy of the model on the 400 val images: 0.455\n",
      "test Accuracy of the model on the 400 test images: 0.415\n",
      "Epoch [20/1000], Step [11/12], Loss: 0.2552\n",
      "train Accuracy of the model on the 1200 train images: 0.873\n",
      "val Accuracy of the model on the 400 val images: 0.455\n",
      "test Accuracy of the model on the 400 test images: 0.422\n",
      "Epoch [21/1000], Step [1/12], Loss: 0.3763\n",
      "train Accuracy of the model on the 1200 train images: 0.778\n",
      "val Accuracy of the model on the 400 val images: 0.468\n",
      "test Accuracy of the model on the 400 test images: 0.445\n",
      "Epoch [21/1000], Step [11/12], Loss: 0.2915\n",
      "train Accuracy of the model on the 1200 train images: 0.912\n",
      "val Accuracy of the model on the 400 val images: 0.425\n",
      "test Accuracy of the model on the 400 test images: 0.448\n",
      "Epoch [22/1000], Step [1/12], Loss: 0.2192\n",
      "train Accuracy of the model on the 1200 train images: 0.890\n",
      "val Accuracy of the model on the 400 val images: 0.430\n",
      "test Accuracy of the model on the 400 test images: 0.485\n",
      "Epoch [22/1000], Step [11/12], Loss: 0.1386\n",
      "train Accuracy of the model on the 1200 train images: 0.862\n",
      "val Accuracy of the model on the 400 val images: 0.400\n",
      "test Accuracy of the model on the 400 test images: 0.440\n",
      "Epoch [23/1000], Step [1/12], Loss: 0.4480\n",
      "train Accuracy of the model on the 1200 train images: 0.928\n",
      "val Accuracy of the model on the 400 val images: 0.388\n",
      "test Accuracy of the model on the 400 test images: 0.448\n",
      "Epoch [23/1000], Step [11/12], Loss: 0.1341\n",
      "train Accuracy of the model on the 1200 train images: 0.958\n",
      "val Accuracy of the model on the 400 val images: 0.422\n",
      "test Accuracy of the model on the 400 test images: 0.432\n",
      "Epoch [24/1000], Step [1/12], Loss: 0.1271\n",
      "train Accuracy of the model on the 1200 train images: 0.959\n",
      "val Accuracy of the model on the 400 val images: 0.448\n",
      "test Accuracy of the model on the 400 test images: 0.425\n",
      "Epoch [24/1000], Step [11/12], Loss: 0.1572\n",
      "train Accuracy of the model on the 1200 train images: 0.971\n",
      "val Accuracy of the model on the 400 val images: 0.448\n",
      "test Accuracy of the model on the 400 test images: 0.460\n",
      "Epoch [25/1000], Step [1/12], Loss: 0.0756\n",
      "train Accuracy of the model on the 1200 train images: 0.983\n",
      "val Accuracy of the model on the 400 val images: 0.432\n",
      "test Accuracy of the model on the 400 test images: 0.417\n",
      "Epoch [25/1000], Step [11/12], Loss: 0.0696\n",
      "train Accuracy of the model on the 1200 train images: 0.997\n",
      "val Accuracy of the model on the 400 val images: 0.420\n",
      "test Accuracy of the model on the 400 test images: 0.458\n",
      "Epoch [26/1000], Step [1/12], Loss: 0.0467\n",
      "train Accuracy of the model on the 1200 train images: 0.970\n",
      "val Accuracy of the model on the 400 val images: 0.453\n",
      "test Accuracy of the model on the 400 test images: 0.472\n",
      "Epoch [26/1000], Step [11/12], Loss: 0.1260\n",
      "train Accuracy of the model on the 1200 train images: 0.991\n",
      "val Accuracy of the model on the 400 val images: 0.390\n",
      "test Accuracy of the model on the 400 test images: 0.425\n",
      "Epoch [27/1000], Step [1/12], Loss: 0.0335\n",
      "train Accuracy of the model on the 1200 train images: 0.990\n",
      "val Accuracy of the model on the 400 val images: 0.422\n",
      "test Accuracy of the model on the 400 test images: 0.422\n",
      "Epoch [27/1000], Step [11/12], Loss: 0.1145\n",
      "train Accuracy of the model on the 1200 train images: 0.925\n",
      "val Accuracy of the model on the 400 val images: 0.417\n",
      "test Accuracy of the model on the 400 test images: 0.422\n",
      "Epoch [28/1000], Step [1/12], Loss: 0.1888\n",
      "train Accuracy of the model on the 1200 train images: 0.988\n",
      "val Accuracy of the model on the 400 val images: 0.385\n",
      "test Accuracy of the model on the 400 test images: 0.432\n",
      "Epoch [28/1000], Step [11/12], Loss: 0.0665\n",
      "train Accuracy of the model on the 1200 train images: 0.929\n",
      "val Accuracy of the model on the 400 val images: 0.415\n",
      "test Accuracy of the model on the 400 test images: 0.453\n",
      "Epoch [29/1000], Step [1/12], Loss: 0.2050\n",
      "train Accuracy of the model on the 1200 train images: 0.906\n",
      "val Accuracy of the model on the 400 val images: 0.380\n",
      "test Accuracy of the model on the 400 test images: 0.443\n",
      "Epoch [29/1000], Step [11/12], Loss: 0.2298\n",
      "train Accuracy of the model on the 1200 train images: 0.980\n",
      "val Accuracy of the model on the 400 val images: 0.420\n",
      "test Accuracy of the model on the 400 test images: 0.410\n",
      "Epoch [30/1000], Step [1/12], Loss: 0.0358\n",
      "train Accuracy of the model on the 1200 train images: 0.963\n",
      "val Accuracy of the model on the 400 val images: 0.410\n",
      "test Accuracy of the model on the 400 test images: 0.427\n",
      "Epoch [30/1000], Step [11/12], Loss: 0.2459\n",
      "train Accuracy of the model on the 1200 train images: 0.979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49097/1629480624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_49097/1629480624.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, cnn, loaders)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 cnn_epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_cnn(\"train\"), \n\u001b[0;32m---> 35\u001b[0;31m                              \"val\": eval_cnn(\"val\"), \"test\": eval_cnn(\"test\")}\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49097/4076580431.py\u001b[0m in \u001b[0;36meval_cnn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mpred_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_ys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mlabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 1000\n",
    "cnn_epochs = {}\n",
    "\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            images = images.reshape(-1,40,40,100).to(device)\n",
    "            labels = labels.to(device)\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step()                \n",
    "            #print(i)\n",
    "            if (i) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "                cnn_epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_cnn(\"train\"), \n",
    "                             \"val\": eval_cnn(\"val\"), \"test\": eval_cnn(\"test\")}\n",
    "                cnn.train()\n",
    "                pass\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057b15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de745a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a8046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e90241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1d7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bc88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899bba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd2587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eaf27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "012a18d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49763/2954024906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dynamic add padding based on the kernel_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mconv3x3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2dAuto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4cf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
